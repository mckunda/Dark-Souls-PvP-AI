FANN_FLO_2.1
num_layers=10
learning_rate=0.700000
connection_rate=1.000000
network_type=1
learning_momentum=0.000000
training_algorithm=2
train_error_function=0
train_stop_function=1
cascade_output_change_fraction=0.010000
quickprop_decay=-0.000100
quickprop_mu=1.750000
rprop_increase_factor=1.200000
rprop_decrease_factor=0.500000
rprop_delta_min=0.000000
rprop_delta_max=50.000000
rprop_delta_zero=0.100000
cascade_output_stagnation_epochs=12
cascade_candidate_change_fraction=0.010000
cascade_candidate_stagnation_epochs=12
cascade_max_out_epochs=150
cascade_min_out_epochs=50
cascade_max_cand_epochs=150
cascade_min_cand_epochs=50
cascade_num_candidate_groups=2
bit_fail_limit=8.99999976158142090000e-001
cascade_candidate_limit=1.00000000000000000000e+003
cascade_weight_multiplier=4.00000005960464480000e-001
cascade_activation_functions_count=10
cascade_activation_functions=3 5 7 8 10 11 14 15 16 17 
cascade_activation_steepnesses_count=4
cascade_activation_steepnesses=2.50000000000000000000e-001 5.00000000000000000000e-001 7.50000000000000000000e-001 1.00000000000000000000e+000 
layer_sizes=5 1 1 1 1 1 1 1 1 1 
scale_included=0
neurons (num_inputs, activation_function, activation_steepness)=(0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (0, 0, 0.00000000000000000000e+000) (5, 16, 1.00000000000000000000e+000) (6, 14, 5.00000000000000000000e-001) (7, 15, 1.00000000000000000000e+000) (8, 15, 1.00000000000000000000e+000) (9, 8, 5.00000000000000000000e-001) (10, 14, 5.00000000000000000000e-001) (11, 14, 7.50000000000000000000e-001) (12, 14, 7.50000000000000000000e-001) (13, 0, 5.00000000000000000000e-001) 
connections (connected_to_neuron, weight)=(0, 3.39646637439727780000e-001) (1, 3.85764694213867190000e+000) (2, -1.12703299522399900000e+000) (3, 7.28152468800544740000e-002) (4, 3.52972388267517090000e-001) (0, 9.35138225555419920000e+000) (1, -2.34423112869262700000e+000) (2, 3.61794900894165040000e+000) (3, 1.02996940612792970000e+001) (4, -1.87385916709899900000e+000) (5, 6.06234512329101560000e+001) (0, 1.41308164596557620000e+000) (1, 1.20974421501159670000e+000) (2, 6.45395994186401370000e-001) (3, 1.04182958602905270000e+001) (4, 3.30657124519348140000e+000) (5, 4.23509712219238280000e+001) (6, 1.95359587669372560000e+000) (0, 4.11577701568603520000e+000) (1, 1.74465489387512210000e+000) (2, 2.96550774574279790000e+000) (3, -3.28318333625793460000e+000) (4, 1.32354950904846190000e+000) (5, 5.60616874694824220000e+001) (6, 1.70073640346527100000e+000) (7, 6.01735782623291020000e+000) (0, -1.98402595520019530000e+000) (1, 4.09156703948974610000e+000) (2, 2.65820407867431640000e+000) (3, -3.77549338340759280000e+000) (4, 1.47334432601928710000e+000) (5, 6.29696044921875000000e+001) (6, -2.05760622024536130000e+000) (7, 3.03082251548767090000e+000) (8, 3.39473056793212890000e+000) (0, 1.23230004310607910000e+000) (1, 2.83863401412963870000e+000) (2, 3.47450256347656250000e+000) (3, -2.35305762290954590000e+000) (4, -2.35266542434692380000e+000) (5, -2.76927909851074220000e+001) (6, 2.65764594078063960000e+000) (7, 3.67128300666809080000e+000) (8, -3.96500468254089360000e+000) (9, 5.94588375091552730000e+000) (0, 2.27023744583129880000e+000) (1, 2.87576627731323240000e+000) (2, 2.16404247283935550000e+000) (3, -1.38184535503387450000e+000) (4, -1.47786724567413330000e+000) (5, 1.15002851486206050000e+001) (6, -4.03275005519390110000e-002) (7, 8.89051020145416260000e-001) (8, 3.71212434768676760000e+000) (9, 4.54828977584838870000e+000) (10, -4.04136389493942260000e-001) (0, -9.22447919845581050000e-001) (1, 5.10699605941772460000e+000) (2, 2.32993125915527340000e+000) (3, -3.72854709625244140000e-001) (4, -2.58513903617858890000e+000) (5, 5.06867332458496090000e+001) (6, -2.03783845901489260000e+000) (7, 2.46952414512634280000e+000) (8, 1.98730480670928960000e+000) (9, 1.53202188014984130000e+000) (10, 2.04311513900756840000e+000) (11, 1.87607133388519290000e+000) (0, -3.67720514535903930000e-001) (1, 3.06153059005737300000e+000) (2, 8.62822681665420530000e-002) (3, -9.38228517770767210000e-003) (4, 2.78646612167358400000e+000) (5, 2.32587089538574220000e+001) (6, -1.01569068431854250000e+000) (7, 1.18981134891510010000e+000) (8, 7.69873201847076420000e-001) (9, 6.70631945133209230000e-001) (10, 1.06508898735046390000e+000) (11, 9.17597770690917970000e-001) (12, 1.05402290821075440000e+000) 
